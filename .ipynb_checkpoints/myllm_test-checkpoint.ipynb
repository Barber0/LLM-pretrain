{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed867fe2-30e7-45b5-b82d-ffc7795b96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a1a28f-1cf0-43cc-b558-5c1b97c33286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from mytkn import get_tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46a80d8-7e5e-4b8f-9a15-0bdd3210b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn, VOCAB_SIZE, START_SIGN, END_SIGN, START_ID, END_ID = get_tkn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081c622e-9261-4b6f-8b9d-001daa548cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "    display(HTML('''\n",
    "    <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "    </style>\n",
    "    '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119ed14b-58a3-4a92-9ea0-3946a633ff63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from myllm_model import MyModel\n",
    "model = MyModel(\n",
    "    vocab=VOCAB_SIZE,\n",
    "    pad_token_id=tkn.pad_token_id,\n",
    "    d_model=1280,\n",
    "    num_head=32,\n",
    "    num_block=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd998195-8527-41d1-815e-1cc50cfb2179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['module', 'buffer_names', 'optimizer', 'param_shapes', 'frozen_param_shapes', 'shared_params', 'frozen_param_fragments', 'lr_scheduler', 'data_sampler', 'random_ltd', 'sparse_tensor_module_names', 'skipped_steps', 'global_steps', 'global_samples', 'dp_world_size', 'mp_world_size', 'ds_config', 'ds_version'])\n"
     ]
    }
   ],
   "source": [
    "chkpt = torch.load('../mp_rank_00_model_states.pt')\n",
    "model.load_state_dict(chkpt['module'])\n",
    "model.eval()\n",
    "print(chkpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2592948f-0755-4f6c-a311-e0874829af77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, context, length, tokenizer, temperature=1.0, top_k=30, top_p=0.0, repitition_penalty=1.0,\n",
    "                    device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    inputs = context\n",
    "\n",
    "    output = None\n",
    "    prefix_kv_list = None\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length - context.size(1)):\n",
    "            model_o, prefix_kv_list = model(inputs, prefix_kv_list=prefix_kv_list)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = model_o[0, -1, :]\n",
    "\n",
    "            if output is not None:\n",
    "                for tmp_id in set(output[0]):\n",
    "                    next_token_logits[tmp_id] /= repitition_penalty\n",
    "\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            next_token_logits[tkn.bos_token_id] = -float('Inf')\n",
    "            next_token_logits[tkn.eos_token_id] = -float('Inf')\n",
    "            next_token_logits[tkn.unk_token_id] = -float('Inf')\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            if next_token == END_ID:\n",
    "                break\n",
    "\n",
    "            next_token = next_token.unsqueeze(0)\n",
    "            inputs = next_token\n",
    "\n",
    "            print(tokenizer.decode(next_token[0]), end='')\n",
    "            if output is None:\n",
    "                output = next_token\n",
    "            else:\n",
    "                output = torch.cat((output, next_token), dim=1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b42b7e-0579-4999-a629-10ea05a8c828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def answer(prompt):\n",
    "    print(prompt, end='')\n",
    "    context_tokens = tkn.convert_tokens_to_ids(tkn.tokenize(f'''{START_SIGN} {prompt}'''))\n",
    "    out = sample_sequence(\n",
    "      model=model, length=512,\n",
    "      context=context_tokens, tokenizer=tkn,\n",
    "      temperature=0.9, top_k=30, repitition_penalty=5\n",
    "    )\n",
    "    clear_output()\n",
    "    print(prompt, tkn.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8ccda07-06e4-484b-b61e-7fb5947c0566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Please tell me why did Bill Gates retire? Output:  \n",
      "Person: Bill Gates are a billionaire. He is the founder of Microsoft Corporation (MSFT), and CEO at Google Inc., said in an interview with Coding USA on September 5th, 2011 that he became \"a successful entrepreneur\" after his retirement from eBay's board according to its description.\" George Washington\n"
     ]
    }
   ],
   "source": [
    "answer('''Input: Please tell me why did Bill Gates retire? '''\n",
    "       '''Output: ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa8d6c-26d1-43c8-9530-76d7c2412e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
