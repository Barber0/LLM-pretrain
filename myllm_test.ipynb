{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed867fe2-30e7-45b5-b82d-ffc7795b96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a1a28f-1cf0-43cc-b558-5c1b97c33286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import prepare_tokenizer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46a80d8-7e5e-4b8f-9a15-0bdd3210b3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tkn, VOCAB_SIZE = prepare_tokenizer('./tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081c622e-9261-4b6f-8b9d-001daa548cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "    display(HTML('''\n",
    "    <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "    </style>\n",
    "    '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119ed14b-58a3-4a92-9ea0-3946a633ff63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rope_model import LLM\n",
    "model = LLM(\n",
    "    vocab=VOCAB_SIZE,\n",
    "    pad_token_id=tkn.pad_token_id,\n",
    "    d_model=2560,\n",
    "    num_head=32,\n",
    "    num_block=24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd998195-8527-41d1-815e-1cc50cfb2179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['module', 'buffer_names', 'optimizer', 'param_shapes', 'frozen_param_shapes', 'shared_params', 'frozen_param_fragments', 'lr_scheduler', 'data_sampler', 'random_ltd', 'sparse_tensor_module_names', 'skipped_steps', 'global_steps', 'global_samples', 'dp_world_size', 'mp_world_size', 'ds_config', 'ds_version'])\n"
     ]
    }
   ],
   "source": [
    "chkpt = torch.load('/root/autodl-tmp/backup-models/myllm4-2B-wiki-rope-90000.pt')\n",
    "model = model.cpu()\n",
    "model.load_state_dict(chkpt['module'])\n",
    "model.eval()\n",
    "print(chkpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2592948f-0755-4f6c-a311-e0874829af77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, context, length, tokenizer, temperature=1.0, top_k=30, top_p=0.0, repitition_penalty=1.0,\n",
    "                    device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    inputs = context\n",
    "\n",
    "    output = None\n",
    "    prefix_kv_list = None\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length - context.size(1)):\n",
    "            model_o, prefix_kv_list = model(inputs, prefix_kv_list=prefix_kv_list)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = model_o[0, -1, :]\n",
    "\n",
    "            if output is not None:\n",
    "                for tmp_id in set(output[0]):\n",
    "                    next_token_logits[tmp_id] /= repitition_penalty\n",
    "\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            next_token_logits[tkn.bos_token_id] = -float('Inf')\n",
    "            next_token_logits[tkn.eos_token_id] = -float('Inf')\n",
    "            next_token_logits[tkn.unk_token_id] = -float('Inf')\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "\n",
    "            next_token = next_token.unsqueeze(0)\n",
    "            inputs = next_token\n",
    "\n",
    "            print(tokenizer.decode(next_token[0]), end='')\n",
    "            if output is None:\n",
    "                output = next_token\n",
    "            else:\n",
    "                output = torch.cat((output, next_token), dim=1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b42b7e-0579-4999-a629-10ea05a8c828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def answer(prompt):\n",
    "    print(prompt, end='')\n",
    "    context_tokens = tkn.convert_tokens_to_ids(tkn.tokenize(prompt))\n",
    "    out = sample_sequence(\n",
    "      model=model, length=512,\n",
    "      context=context_tokens, tokenizer=tkn,\n",
    "      temperature=0.9, top_k=30, repitition_penalty=5\n",
    "    )\n",
    "    clear_output()\n",
    "    print(prompt, tkn.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ccda07-06e4-484b-b61e-7fb5947c0566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter Parker  (born September 16, 1994. A full member was announced the same year and finished when he has been replaced in a three new to his own time for several major changes; some of all members on 25 years ago.\"\n",
      "The following which they also see each occasioned at one as well-numbered players have won't previously unites with \"the originality missing over other artists remaining words by this number 2Discoveryally before thematic differences were notches into an active until their name change many more than justifications may bearers' namespaces or 'A2 persons are known outside world's worth five timespanorably infinite again! In order). One is unknown\" without reference work permits willoworks that had its age groups  = 888%\" | The others) it requires about 1–presenter symbolized offload from later:\n",
      "\n",
      "\ta no longer list upsurveldiverts only 24 hours because those listed heretics while playing cards under two forms... butchers || – even harder numbers 1371 characters - although therefor example/hearts instead Of multiple factors such games generally do everything else lengthwise depending upon you can’\", including previous albums? Theresectors\". It does something similar conditions inside different categories available after schoolteamsisibility would like most recently completed within these tables used elsewhere), yet another termitella On topology classes — except hermianexpressly bearsimand then any furtheringleseedenlisted if weavers who appeared together.) As fewestimated according non grates faster among women should consistories being included below where someone whose jobable backdated every formersions occur between episodes above 20 October 3200+inities include never accruentsiaxisting. Sincerely what follows + 6 months passagglers normally associated during present good examples downgraded unless otherwise indicated through high enough likely meaning=\"texts individual candidates iDEN3 (\"if type [butaned outcastelements(takeshomes contained therein):... events perfunctions usually & 10−542ndumsily due diligence could happenings might choose howitzvoted soviated differently 463 ...\"|} acting entirely absentialikhsignified since 2005 levels 7½ning lessened}}</refnalatable whether anyone shows] EWiogenicus firmness1 point system winsuition dates existenteredigionarily=see pages cleanup entries rather vests occurs sometimes considered themselves mean feathereditary codes nor coexistence days pastelsasional[...] For\n"
     ]
    }
   ],
   "source": [
    "answer('''Peter Parker''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa8d6c-26d1-43c8-9530-76d7c2412e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
