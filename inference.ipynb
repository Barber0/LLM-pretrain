{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed867fe2-30e7-45b5-b82d-ffc7795b96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a1a28f-1cf0-43cc-b558-5c1b97c33286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import prepare_tokenizer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46a80d8-7e5e-4b8f-9a15-0bdd3210b3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tkn, VOCAB_SIZE = prepare_tokenizer('./tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081c622e-9261-4b6f-8b9d-001daa548cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "    display(HTML('''\n",
    "    <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "    </style>\n",
    "    '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119ed14b-58a3-4a92-9ea0-3946a633ff63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from base_model2 import MyModel\n",
    "model = MyModel(\n",
    "    vocab=VOCAB_SIZE,\n",
    "    pad_token_id=tkn.pad_token_id,\n",
    "    d_model=2560,\n",
    "    num_head=32,\n",
    "    num_block=24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd998195-8527-41d1-815e-1cc50cfb2179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['module', 'buffer_names', 'optimizer', 'param_shapes', 'frozen_param_shapes', 'shared_params', 'frozen_param_fragments', 'lr_scheduler', 'data_sampler', 'random_ltd', 'sparse_tensor_module_names', 'skipped_steps', 'global_steps', 'global_samples', 'dp_world_size', 'mp_world_size', 'ds_config', 'ds_version'])\n"
     ]
    }
   ],
   "source": [
    "chkpt = torch.load('/root/autodl-tmp/myllm/main/mp_rank_00_model_states.pt')\n",
    "model.load_state_dict(chkpt['module'])\n",
    "model.eval()\n",
    "print(chkpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2592948f-0755-4f6c-a311-e0874829af77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, context, length, tokenizer, temperature=1.0, top_k=30, top_p=0.0, repitition_penalty=1.0,\n",
    "                    device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    inputs = context\n",
    "\n",
    "    output = None\n",
    "    prefix_kv_list = None\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length - context.size(1)):\n",
    "            model_o, prefix_kv_list = model(inputs, prefix_kv_list=prefix_kv_list, generate=True)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = model_o[0, -1, :]\n",
    "\n",
    "            if output is not None:\n",
    "                for tmp_id in set(output[0]):\n",
    "                    next_token_logits[tmp_id] /= repitition_penalty\n",
    "\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            next_token_logits[tkn.bos_token_id] = -float('Inf')\n",
    "            next_token_logits[tkn.eos_token_id] = -float('Inf')\n",
    "            next_token_logits[tkn.unk_token_id] = -float('Inf')\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "\n",
    "            next_token = next_token.unsqueeze(0)\n",
    "            inputs = next_token\n",
    "\n",
    "            print(tokenizer.decode(next_token[0]), end='')\n",
    "            if output is None:\n",
    "                output = next_token\n",
    "            else:\n",
    "                output = torch.cat((output, next_token), dim=1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b42b7e-0579-4999-a629-10ea05a8c828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def answer(prompt):\n",
    "    print(prompt, end='')\n",
    "    context_tokens = tkn.convert_tokens_to_ids(tkn.tokenize(prompt))\n",
    "    out = sample_sequence(\n",
    "      model=model, length=512,\n",
    "      context=context_tokens, tokenizer=tkn,\n",
    "      temperature=0.9, top_k=30, repitition_penalty=5\n",
    "    )\n",
    "    clear_output()\n",
    "    print(prompt, tkn.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8ccda07-06e4-484b-b61e-7fb5947c0566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Gates is a successful man, because of his love affair with the evil who is alive by a boy. He does not take part in many ways, and after one night he falls apart from her father's own bodyguard when she finds him at gunpoint for it to be able if Gates was taken out on an island which has been seen as being possessed back up under super-trap before rehearsing them behind their fate (\"Gate Dog\").\n",
      "When Vaday tells Kase that Unti should become Boss while trying so they are going into bedside without making revenge off Tiscalo (the latter also named \"The Last\") due only time lying down; because Nitles had never gone too distant away again until later times were killed or wounded during World War I like all other weapons but this appears against humanity rather than having any reason why Mishna did indeed save others – some people have sex about how well known what happened towards life imprisonment\". Despite Hinduism  This way Paisyam says Edo Chaharma wrote The New Testament called: “Kathuwa”   → ‘Uklawi’ (= [Ra] – means something we believe nowhere — meaning himself said.... One case then mentions Fukhuizen ('Yakkuhi)  [2015') whose name comes next through Dizhuto 'Irka'. Because Others do get ridicies between Sapajunanaka kura & Rālumi Nakagara no more - whether Phraja Kiyoya / Arygamma/ *47], Lulu Naida\t(NOTai Niño), Kaima Fuichi Maiawa ... . It belongs Soho Washiri ni Maumeana : Yekkejee Kuki Takagi kauneatimoto Shōkyūdou Sakogurtii Misuji ga Keisu! | Doomalika Shinaga okauwang Ninjung Minushinori || − Oh Jamaṃ Seko|| Yeonokie Komarami Sangha''               =Diksha Tanitsuhita Prefecture| yenmu Hayashi... Only you can find each individual.\" Includes these two versions together would see Zevigata Kobussi ~ Tochigi Yamaya Tamaki + Hoasda Zabiro Kyuluma da Maribnato Kamari‘ei Ryuki Kim\n"
     ]
    }
   ],
   "source": [
    "answer('''Bill Gates is a successful man, because''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa8d6c-26d1-43c8-9530-76d7c2412e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
