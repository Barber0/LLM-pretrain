{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed867fe2-30e7-45b5-b82d-ffc7795b96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a1a28f-1cf0-43cc-b558-5c1b97c33286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import prepare_tokenizer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46a80d8-7e5e-4b8f-9a15-0bdd3210b3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tkn, VOCAB_SIZE = prepare_tokenizer('./tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081c622e-9261-4b6f-8b9d-001daa548cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from IPython.display import HTML, display\n",
    "\n",
    "# def set_css():\n",
    "#     display(HTML('''\n",
    "#     <style>\n",
    "#     pre {\n",
    "#         white-space: pre-wrap;\n",
    "#     }\n",
    "#     </style>\n",
    "#     '''))\n",
    "# get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ed14b-58a3-4a92-9ea0-3946a633ff63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from base_model import MyModel\n",
    "model = MyModel(\n",
    "    vocab=VOCAB_SIZE,\n",
    "    pad_token_id=tkn.pad_token_id,\n",
    "    d_model=2560,\n",
    "    num_head=32,\n",
    "    num_block=24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd998195-8527-41d1-815e-1cc50cfb2179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['module', 'buffer_names', 'optimizer', 'param_shapes', 'frozen_param_shapes', 'shared_params', 'frozen_param_fragments', 'lr_scheduler', 'data_sampler', 'random_ltd', 'sparse_tensor_module_names', 'skipped_steps', 'global_steps', 'global_samples', 'dp_world_size', 'mp_world_size', 'ds_config', 'ds_version'])\n"
     ]
    }
   ],
   "source": [
    "chkpt = torch.load('/root/autodl-tmp/wront-ft-alpaca/main/mp_rank_00_model_states.pt')\n",
    "model.load_state_dict(chkpt['module'])\n",
    "model.eval()\n",
    "print(chkpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2592948f-0755-4f6c-a311-e0874829af77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, context, length, tokenizer, min_length=20, temperature=1.0, top_k=30, top_p=0.0, repitition_penalty=1.0,\n",
    "                    device='cpu'):\n",
    "    context = context.long().to(device)\n",
    "    # context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    inputs = context\n",
    "\n",
    "    output = None\n",
    "    prefix_kv_list = None\n",
    "    with torch.no_grad():\n",
    "        for i in trange(length - context.size(1)):\n",
    "            model_o, prefix_kv_list = model(inputs, prefix_kv_list=prefix_kv_list, generate=True)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = model_o[0, -1, :]\n",
    "\n",
    "            if output is not None:\n",
    "                for tmp_id in set(output[0]):\n",
    "                    next_token_logits[tmp_id] /= repitition_penalty\n",
    "\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            next_token_logits[tkn.bos_token_id] = -float('Inf')\n",
    "\n",
    "            if output is None or output.size(-1) < min_length:\n",
    "                next_token_logits[tkn.eos_token_id] = -float('Inf')\n",
    "                \n",
    "            next_token_logits[tkn.unk_token_id] = -float('Inf')\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            next_token = next_token.unsqueeze(0)\n",
    "            inputs = next_token\n",
    "\n",
    "            if output is None:\n",
    "                output = next_token\n",
    "            else:\n",
    "                output = torch.cat((output, next_token), dim=1)\n",
    "\n",
    "            txt_gen = tokenizer.decode(output[0])\n",
    "            if txt_gen.endswith(tkn.eos_token):\n",
    "                break\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14b42b7e-0579-4999-a629-10ea05a8c828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer(model, tokenizer, prompt):\n",
    "    print(prompt)\n",
    "    context_tokens = tokenizer(f'{prompt} </s>', return_tensors='pt').input_ids[0]\n",
    "    out = sample_sequence(\n",
    "      model=model, length=512,\n",
    "      context=context_tokens, tokenizer=tkn,\n",
    "      temperature=1, top_k=10, repitition_penalty=10\n",
    "    )\n",
    "    clear_output()\n",
    "    txt_gen = tkn.decode(out[0])\n",
    "    # if txt_gen.endswith(tokenizer.eos_token):\n",
    "    #     txt_gen = txt_gen[:-len(tokenizer.eos_token)]\n",
    "    print(prompt, '\\n')\n",
    "    print(txt_gen.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8ccda07-06e4-484b-b61e-7fb5947c0566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please give me some cooking advice. \n",
      "\n",
      "Here is some cooking advice:\n",
      "- Have a delicious day. ½ cup! Start off using the ground in season one, when you go camp on your way to find it and take down all sorts of food from nearby villages?  2 eggs (theirbs), beans or cranberry juice; get two slices onto another medium bowl with oil that are then heated through an enchanted water supply system like olive branch lines for added nutritional yeast butter spaghetti noodles into powdered vegetables/animal cubes such as chicken broths at night’doughy baking sheet). Tall trees can also bring out their flavorful sweetness so they create more peacefully suited ingredients which keep me entertained during my visit throughout Italy where I'm looking forward again - just want this wonderful spotlightening place before sunny days away.”\n"
     ]
    }
   ],
   "source": [
    "answer(model, tkn, '''Please give me some cooking advice.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a537f-60db-4907-a138-53c68ec3106e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
